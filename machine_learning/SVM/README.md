# HOG+SVM原理
## 一、HOG特征提取
### 1、HOG特征简介
HOG特征是一种图像局部特征，其基本思路是对图像局部的梯度幅值和方向进行投票统计，形成基于梯度特性的直方图，然后将局部特征拼接起来作为总特征。局部特征在这里指的是将图像划分为多个子块（Block), 每个Block内的特征进行联合以形成最终的特征。

HOG+SVM的工作流程如下：

![](https://pic1.zhimg.com/80/v2-057ab6467d4a40a66a03b7e067549a20_1440w.png)
HOG的工作流程如下：

1）灰度化（将图像看做一个x,y,z（灰度）的三维图像）；

2）采用Gamma校正法对输入图像进行颜色空间的标准化（归一化）；目的是调节图像的对比度，降低图像局部的阴影和光照变化所造成的影响，同时可以抑制噪音的干扰；

3）计算图像每个像素的梯度（包括大小和方向）；主要是为了捕获轮廓信息，同时进一步弱化光照的干扰。

4）将图像划分成小cells（例如6*6像素/cell）；

5）统计每个cell的梯度直方图（不同梯度的个数），即可形成每个cell的descriptor；

6）将每几个cell组成一个block（例如3*3个cell/block），一个block内所有cell的特征descriptor串联起来便得到该block的HOG特征descriptor。

7）将图像image内的所有block的HOG特征descriptor串联起来就可以得到该image（你要检测的目标）的HOG特征descriptor了。这个就是最终的可供分类使用的特征向量了。

![](https://img-blog.csdn.net/20170520170209853)

首先对输入的图片进行预处理，然后计算像素点的梯度特特性，包括梯度幅值和梯度方向。然后投票统计形成梯度直方图，然后对blocks进行normalize，最后收集到HOG feature（其实是一行多维的vector）放到SVM里进行监督学习，从而实现行人的检测。
### 2、HOG特征原理

#### (1).图像预处理

预处理包括灰度化和Gamma变换。
灰度处理是可选操作，因为灰度图像和彩色图像都可以用于计算梯度图。对于彩色图像，先对三通道颜色值分别计算梯度，然后取梯度值最大的那个作为该像素的梯度。
然后进行伽马矫正，调节图像对比度，减少光照对图像的影响（包括光照不均和局部阴影），使过曝或者欠曝的图像恢复正常，更接近人眼看到的图像。

**伽马矫正公式：** $\hat{I}(x,y)=I(x,y)^{gamma}$,$(x,y)$ 表示图像，$gamma$ 表示幂指数。

![](https://pic4.zhimg.com/80/v2-ba27373313684e8cbf7f708b63757917_1440w.jpg)

如图，当 $gamma$ 取不同的值时对应的输入输出曲线( $gamma=1$ 时输入输出保持一致) ： 
+ 1） 当 $gamma<1$ 时，输入图像的低灰度值区域动态范围变大，进而图像低灰度值区域对比度得以增强；在高灰度值区域，动态范围变小，进而图像高灰度值区域对比度得以降低。 最终，图像整体的灰度变亮。
+ 2） 当 $gamma>1$ 时，输入图像的低灰度值区域动态范围变小，进而图像低灰度值区域对比度得以降低；在高灰度值区域，动态范围变大，进而图像高灰度值区域对比度得以增强。 最终，图像整体的灰度变暗。


#### (2).计算图像梯度
为了得到梯度直方图，那么首先需要计算图像水平方向和垂直方向梯度。 一般使用特定的卷积核对图像滤波实现，可选用的卷积模板有：soble算子、Prewitt算子、Roberts模板等等。

一般采用soble算子，OpenCV也是如此，利用soble水平和垂直算子与输入图像卷积计算 $dx$、$dy$：
![](https://img-blog.csdn.net/20170520170604021)

最常用的方法是：首先用[-1,0,1]梯度算子对原图像做卷积运算，得到x方向（水平方向，以向右为正方向）的梯度分量 $Sobel_x$ ，然后用[1,0,-1]T梯度算子对原图像做卷积运算，得到y方向（竖直方向，以向上为正方向）的梯度分量 $Sobel_y$ 。然后再用以上公式计算该像素点的梯度大小和方向。

![](https://www.zhihu.com/equation?tex=%5Cbegin%7Barray%7D%7Bl%7D+%5Ctext+%7BSobel%7D_%7BX%7D%3D%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D+1+%5C%5C+0+%5C%5C+-1+%5Cend%7Barray%7D%5Cright%5D+%2A%5Cleft%5B%5Cbegin%7Barray%7D%7Bccc%7D+1+%26+2+%26+1+%5Cend%7Barray%7D%5Cright%5D%3D%5Cleft%5B%5Cbegin%7Barray%7D%7Bccc%7D+1+%26+2+%26+1+%5C%5C+0+%26+0+%26+0+%5C%5C+-1+%26+-2+%26+-1+%5Cend%7Barray%7D%5Cright%5D+%5C%5C+%5Coperatorname%7BSobel%7D_%7BY%7D%3D%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D+1+%5C%5C+2+%5C%5C+1+%5Cend%7Barray%7D%5Cright%5D+%2A%5Cleft%5B%5Cbegin%7Barray%7D%7Bccc%7D+1+%26+0+%26+-1+%5Cend%7Barray%7D%5Cright%5D%3D%5Cleft%5B%5Cbegin%7Barray%7D%7Bccc%7D+1+%26+0+%26+-1+%5C%5C+2+%26+0+%26+-2+%5C%5C+1+%26+0+%26+-1+%5Cend%7Barray%7D%5Cright%5D+%5C%5C+d_%7Bx%7D%3Df%28x%2C+y%29%5E%7B%2A%7D+%5Coperatorname%7BSobel%7D_%7Bx%7D%28x%2C+y%29+%5C%5C+d_%7By%7D%3Df%28x%2C+y%29%5E%7B%2A%7D+%5Coperatorname%7BSobel%7D_%7By%7D%28x%2C+y%29+%5Cend%7Barray%7D%5C%5C)

这里需要注意的是：梯度方向和图像边缘方向是互相正交的。

![](https://pic1.zhimg.com/80/v2-a9af6872b3dad43895c8e0fc0b8d1738_1440w.jpg)


#### (3).计算梯度直方图

经过上一步计算，每一个像素点都会有两个值：**梯度幅值/梯度方向。**

在这一步中，图像被分成若干个8×8的cell，例如我们将图像resize至64x128的大小，那么这幅图像就被划分为8x16个8x8的cell单元，并为每个8×8的cell计算梯度直方图。当然，cell的划分也可以是其他值：16x16，8x16等，根据具体的场景确定。

在计算梯度直方图，让我们先了解一下为什么我们将图像分成若干个cell?

这是因为如果对一整张梯度图逐像素计算，其中的有效特征是非常稀疏的，不但运算量大，而且会受到一些噪声干扰。于是我们就使用局部特征描述符来表示一个更紧凑的特征，计算这种局部cell上的梯度直方图更具鲁棒性。

以8x8的cell为例，一个8x8的cell包含了8x8x2 = 128个值，因为每个像素包括梯度的大小和方向。

在HOG中，每个8x8的cell的梯度直方图本质是一个由9个数值组成的向量， 对应于0、20、40、60…160的梯度方向(角度)。那么原本cell中8x8x2 = 128个值就由长度为9的向量来表示，用这种梯度直方图的表示方法，大大降低了计算量，同时又对光照等环境变化更加地鲁棒。

![](https://pic1.zhimg.com/80/v2-338b49b78be96fdd0edbc55b142802d4_1440w.jpg)

上图是飞人博尔特64x128的图像，被划分为8x16个8x8的cell；中间的图像表示一个cell中的梯度矢量，箭头朝向代表梯度方向，箭头长度代表梯度大小。

下图是 8×8 的cell中表示梯度的原始数值，注意角度的范围介于0到180度之间，而不是0到360度， 这被称为“无符号”梯度，因为两个完全相反的方向被认为是相同的。**([python程序](https://github.com/SHUNLU-1/Notes/blob/main/machine_learning/SVM/hog.py)中使用360度，此处以180度做解释。可以参考[blog](https://blog.csdn.net/q1007729991/article/details/53032776))**

接下来，我们来计算cell中像素的梯度直方图，将0-180度分成9等份，称为9个bins，分别是0，20，40...160。然后对每个bin中梯度的贡献进行统计：

![](https://pic3.zhimg.com/80/v2-ccf60415c533e67db82469d39f8e81ba_1440w.jpg)

计方法是一种加权投票统计， 如上图所示，某像素的梯度幅值为13.6，方向为36，36度两侧的角度bin分别为20度和40度，那么就按一定加权比例分别在20度和40度对应的bin加上梯度值，加权公式为：

40度对应的bin：(（40-36）/20) * 13.6，分母的20表示20等份，而不是20度； 20度对应的bin：(（36-20）/20) * 13.6，分母的20表示20等份，而不是20度；

还有一个细节需要注意，如果某个像素的梯度角度大于160度，也就是在160度到180度之间，那么把这个像素对应的梯度值按比例分给0度和160度对应的bin。如左下图绿色圆圈中的角度为165度，幅值为85，则按照同样的加权方式将85分别加到0度和160度对应的bin中。

![](https://pic1.zhimg.com/80/v2-fce77f38757afa16f1451df343da42c0_1440w.jpg)

对整个cell进行投票统计，正是在HOG特征描述子中创建直方图的方式，最终得到由9个数值组成的向量—梯度方向图：

![](https://pic4.zhimg.com/80/v2-dc86f6e49c8e40619d39e19d7ba5cc4f_1440w.jpg)

Block 归一化 HOG特征将8×8的一个局部区域作为一个cell，再以2×2个cell作为一组，称为一个block，也就是说一个block表示16x16的区域。

我们可能会想，为什么又需要分block呢？

这是因为，虽然我们已经为图像的8×8单元创建了HOG特征，但是图像的梯度对整体光照很敏感。这意味着对于特定的图像，图像的某些部分与其他部分相比会非常明亮。

我们不能从图像中完全消除这个。但是我们可以通过使用16×16个块来对梯度进行归一化来减少这种光照变化。

由于每个cell有9个值，一个block（2×2个cell）则有36个值，HOG是通过滑动窗口的方式来得到block的.

前面已经说明，归一化的目的是为了降低光照的影响，因为梯度对整体光照非常敏感，比如通过将所有像素值除以2来使图像变暗，那么梯度幅值将减小一半，因此直方图中的值也将减小一半，我们就需要将直方图“归一化”。

归一化的方法有很多：L1-norm、L2-norm、max/min等等，一般选择L2-norm。

例如对于一个[128，64，32]的三维向量来说，模长是[公式]，这叫做向量的L2范数。将这个向量的每个元素除以146.64就得到了归一化向量 [0.87, 0.43, 0.22]。

采用同样的方法，一个cell有一个梯度方向直方图，包含9个数值，一个block有4个cell，那么一个block就有4个梯度方向直方图，将这4个直方图拼接成长度为36的向量，然后对这个向量进行归一化。

而每一个block将按照上图滑动的方式进行重复计算，直到整个图像的block都计算完成。

#### (4).获得HOG描述子

每一个16 * 16大小的block将会得到一个长度为36的特征向量，并进行归一化。 那会得到多少个特征向量呢？

例如，对于上图被划分**8 * 16个cell** ，每个block有**2x2个cell**的话，那么cell的个数为：**(16-1)x(8-1)=105**。即有**7个水平block和15个竖直block**。每个**block有36个**值，整合所有block的特征值，最终获得由**36 * 105=3780个特征值**组成的特征描述符，而这个特征描述符是一个一维的向量，**长度为3780**。获得HOG特征向量，就可以用来可视化和分类了。对于多维的HOG特征，SVM就可以排上用场了。

### 3、HOG特征总结

**HOG算法具有以下优点：**
+ HOG描述的是边缘结构特征，可以描述物体的结构信息
+ 对光照影响不敏感
+ 分块的处理可以使特征得到更为紧凑的表示

**HOG算法具有以下缺点：**
+ 特征描述子获取过程复杂，维数较高，导致实时性差
+ 遮挡问题很难处理
+ 对噪声比较敏感

## 二、SVM原理
### 1、了解SVM

**支持向量机**，因其英文名为support vector machine，故一般简称SVM，通俗来讲，它是一种二类分类模型，其基本模型定义为特征空间上的间隔最大的线性分类器，其学习策略便是间隔最大化，最终可转化为一个凸二次规划问题的求解。
 
### 1.1、分类标准的起源：Logistic回归

**线性分类器**: 给定一些数据点，它们分别属于两个不同的类，现在要找到一个线性分类器把这些数据分成两类。如果用x表示数据点，用y表示类别（y可以取1或者-1，分别代表两个不同的类），一个线性分类器的学习目标便是要在n维的数据空间中找到一个超平面（hyper plane），这个超平面的方程可以表示为（w 是垂直于超平面的一个向量，定义为法向量，而中$w^T$的T代表转置）：
![](https://img-blog.csdn.net/20131107201104906)

Logistic回归目的是从特征学习出一个0/1分类模型，而这个模型是将特性的线性组合作为自变量，由于自变量的取值范围是负无穷到正无穷。因此，使用logistic函数（或称作sigmoid函数）将自变量映射到(0,1)上，映射后的值被认为是属于y=1的概率。

假设函数![](https://img-blog.csdnimg.cn/img_convert/d8a4be6614d6ebc09db2ec89a45d03d1.png)其中x是n维特征向量，函数g就是logistic函数。而![](https://img-blog.csdnimg.cn/img_convert/3679fe3a60bbdbb15f528a8f651e2524.png)的图像是

![](https://img-blog.csdnimg.cn/img_convert/b4f43c5c5e56d0fbcc16e0e0577a13bf.png)

可以看到，将无穷映射到了(0,1)。而假设函数就是特征属于y=1的概率。

![](https://img-blog.csdnimg.cn/img_convert/4ed31212fe6ceb6982bb559fb964f05b.png)

从而，当我们要判别一个新来的特征属于哪个类时，只需求 $h_{\theta}(x)$ 即可，若大于0.5就是y=1的类，反之属于y=0类。

此外，$h_{\theta}(x)$ 只和 $\theta^Tx$ 有关，$\theta^Tx>0$ ，那么，而 $g(z)$ 只是用来映射，真实的类别决定权还是在于 $\theta^Tx$ 。再者，当 $\theta^Tx>>0$ 时，$h_{\theta}(x)$ =1，反之 $h_{\theta}(x)$ =0。如果我们只从出发，希望模型达到的目标就是让训练数据中y=1的特征，而是y=0的特征。Logistic回归就是要学习得到 $\theta$ ，使得正例的特征远大于0，负例的特征远小于0，而且要在全部训练实例上达到这个目标。

接下来，尝试把logistic回归做个变形。首先，将使用的结果标签y = 0和y = 1替换为y = -1,y = 1，然后将 $\theta^Tx = \theta_0+\theta_1x_1+\theta_2x_2+...+\theta_nx_n(x_0=1)$ 中的替换为 $b$，最后将后面的 $\theta_1x_1+\theta_2x_2+...+\theta_nx_n$ 替换为 $w^Tx$ 。如此，则有了 $\theta^Tx=w^Tx+b$ 。也就是说除了y由y=0变为y=-1外，线性分类函数跟logistic回归的形式化表示 $h_{\theta}(x)=g(\theta^Tx)=g(w^Tx+b)$ 没区别。

进一步，可以将假设函数 $h_{w,b}(x)=g(w^Tx+b)$ 中的 $g(z)$ 做一个简化，将其简单映射到y=-1和y=1上。映射关系如下：![](https://img-blog.csdnimg.cn/img_convert/1edf77c3ca84b0ef7950fab23cf41bf3.png)

### 1.2、线性分类的一个例子
如下图所示，现在有一个二维平面，平面上有两种不同的数据，分别用圈和叉表示。由于这些数据是线性可分的，所以可以用一条直线将这两类数据分开，这条直线就相当于一个超平面，超平面一边的数据点所对应的y全是-1 ，另一边所对应的y全是1。

![](https://img-blog.csdn.net/20140829134124453)

这个超平面可以用分类函数![](https://img-blog.csdn.net/20131107201211968)表示，当f(x) 等于0的时候，x便是位于超平面上的点，而f(x)大于0的点对应 y=1 的数据点，f(x)小于0的点对应y=-1的点，如下图所示：

![](https://img-blog.csdn.net/20140829134548371)

有的资料上定义特征到结果的输出函数![](https://img-blog.csdn.net/20180514181903580),与这里定义的![](https://img-blog.csdn.net/20131107201211968)实质是一样的。为什么？因为无论是![](https://img-blog.csdn.net/20131120103601656)，还是![](https://img-blog.csdn.net/20131107201211968)，不影响最终优化结果。下文你将看到，当我们转化到优化![](https://img-blog.csdnimg.cn/img_convert/ea1527fd25dba5634147a06d268934be.png)的时候，为了求解方便，会把 $yf(x)$ 令为1，即 $yf(x)$ 是 $y(w^x + b)$ ，还是 $y(w^x - b)$ ，对我们要优化的式子 $max1/||w||$ 已无影响。

### 1.3、函数间隔Functional margin与几何间隔Geometrical margin 
在超平面 $w*x+b=0$ 确定的情况下，$|w*x+b|$ 能够表示点x到距离超平面的远近，而通过观察 $w*x+b$ 的符号与类标记 $y$ 的符号是否一致可判断分类是否正确，所以，可以用 $(y*(w*x+b))$ 的正负性来判定或表示分类的正确性。于此，我们便引出了函数间隔（functional margin）的概念。

定义函数间隔 $\hat\gamma$（用表示）为：![](https://img-blog.csdn.net/20131107201248921)
而超平面 $(w，b)$ 关于T中所有样本点 $(xi，yi)$ 的函数间隔最小值（其中，x是特征，y是结果标签，i表示第i个样本），便为超平面 $(w, b)$ 关于训练数据集T的函数间隔：$\hat\gamma=min\hat\gamma_i(i=1,...n)$

但这样定义的函数间隔有问题，即如果成比例的改变 $w$ 和 $b$（如将它们改成 $2w$ 和 $2b$），则函数间隔的值 $f(x)$ 却变成了原来的2倍（虽然此时超平面没有改变），所以只有函数间隔还远远不够。事实上，我们可以对法向量 $w$ 加些约束条件，从而引出真正定义点到超平面的距离--几何间隔（geometrical margin）的概念。

假定对于一个点 $x$ ，令其垂直投影到超平面上的对应点为 $x_0$ ，$w$ 是垂直于超平面的一个法向量，$\gamma$ 为样本 $x$ 到超平面的距离，如下图所示：
![](https://img-blog.csdnimg.cn/img_convert/e1cc403aa9a3ced1d7b8f377c064a50f.png)

根据平面几何知识，有![](https://img-blog.csdn.net/20131107201720515)

其中 $||w||$ 为w的二阶范数（范数是一个类似于模的表示长度的概念），![](https://img-blog.csdn.net/20131107201720515)是单位向量（一个向量除以它的模称之为单位向量）。

又由于 $x_0$ 是超平面上的点，满足 $f(x_0)=0$，代入超平面的方程 $w^Tx+b-0$，可得 $w^Tx+b-0$，即 $w^Tx=-b$。 随即让此式![](https://img-blog.csdn.net/20131107201720515)的两边同时乘以 $w^T$ ，再根据 $w^Tx=-b$ 和 $w^Tw=||w||^2$，即可算出 $\gamma$： 
![](https://img-blog.csdn.net/20131107201759093)
为了得到 $\gamma$ 的绝对值，令 $\gamma$ 乘上对应的类别 $y$，即可得出几何间隔（用![](https://img-blog.csdn.net/20140829135609579)表示）的定义：![](https://img-blog.csdn.net/20131107201919484)

从上述函数间隔和几何间隔的定义可以看出：几何间隔就是函数间隔除以 $||w||$ ，即：![](https://img-blog.csdn.net/20131107201919484) 而且函数间隔 $y*(wx+b) = y*f(x)$ 实际上就是 $|f(x)|$，只是人为定义的一个间隔度量，而几何间隔 $|f(x)|/||w||$ 才是直观上的点到超平面的距离。

### 1.4、最大间隔分类器Maximum Margin Classifier的定义
对一个数据点进行分类，当超平面离数据点的“间隔”越大，分类的确信度（confidence）也越大。所以，为了使得分类的确信度尽量高，需要让所选择的超平面能够最大化这个“间隔”值。这个间隔就是下图中的Gap的一半。

![](https://img-blog.csdn.net/20140829135959290)

通过由前面的分析可知：函数间隔不适合用来最大化间隔值，因为在超平面固定以后，可以等比例地缩放 $w$ 的长度和 $b$ 的值，这样可以使得 $f(x)=w^Tx+b$ 的值任意大，亦即函数间隔 $\hat\gamma$ 可以在超平面保持不变的情况下被取得任意大。但几何间隔因为除上了 $||w||$ ，使得在缩放 $w$ 和 $b$ 的时候几何间隔 $\hat\gamma$ 的值是不会改变的，它只随着超平面的变动而变动，因此，这是更加合适的一个间隔。换言之，这里要找的最大间隔分类超平面中的“间隔”指的是几何间隔。
于是最大间隔分类器（maximum margin classifier）的目标函数可以定义为:![](https://img-blog.csdn.net/20131111160612687)
同时需满足一些条件，根据间隔的定义，有![](https://img-blog.csdnimg.cn/img_convert/59eae9344883860dbf1587a2159c9128.png) 其中，s.t.，即subject to的意思，它导出的是约束条件。

回顾下几何间隔的定义![](https://img-blog.csdn.net/20131107201919484)，可知：如果令函数间隔 $\hat\gamma$ 等于1（之所以令 $\hat\gamma$ 等于1，是为了方便推导和优化，且这样做对目标函数的优化没有影响），则有 $\widetilde{\gamma} = 1 / ||w||$ 且![](https://img-blog.csdn.net/20140829140642940)，从而上述目标函数转化成了![](https://img-blog.csdnimg.cn/img_convert/ea1527fd25dba5634147a06d268934be.png)

相当于在相应的约束条件![](https://img-blog.csdn.net/20140829140642940)下，最大化这个 $1/||w||$ 值，而 $1/||w||$ 便是几何间隔 $\widetilde{\gamma}$。

如下图所示，中间的实线便是寻找到的最优超平面（Optimal Hyper Plane），其到两条虚线边界的距离相等，这个距离便是几何间隔 $\widetilde{\gamma}$，两条虚线间隔边界之间的距离等于 $2\widetilde{\gamma}$，而**虚线间隔边界上的点则是支持向量**。由于这些支持向量刚好在虚线间隔边界上，所以它们满足 $y(w^Tx+b)=1$（还记得我们把 functional margin 定为 1 了吗？上节中：处于方便推导和优化的目的，我们可以令 $\widetilde{\gamma}$ =1），而对于所有不是支持向量的点，则显然有 $y(w^Tx+b)=1$。
![](https://img-blog.csdn.net/20140829141714944)

### 2、深入svm
### 2.1、从线性可分到线性不可分
#### 2.1.1、从原始问题到对偶问题的求解
接着考虑之前得到的目标函数：![](https://img-blog.csdnimg.cn/img_convert/d0b8a1c58d146d25767ae1233d114d99.png)
由于求![](https://img-blog.csdnimg.cn/img_convert/c16922de920c93c3641a980d6524f3e3.png)的最大值相当于求![](https://img-blog.csdnimg.cn/img_convert/2b3dd7fdc6f81556e815e5bfefbf8ff8.png)的最小值，所以上述目标函数等价于（ $w$ 由分母变成分子，从而也有原来的max问题变为min问题，很明显，两者问题等价）：![](https://img-blog.csdnimg.cn/img_convert/16becf06e98ce0a57a405d707f0c0ff7.png)
因为现在的目标函数是二次的，约束条件是线性的，所以它是一个凸二次规划问题。这个问题可以用现成的[QP (Quadratic Programming)](https://en.wikipedia.org/wiki/Quadratic_programming) 优化包进行求解。一言以蔽之：在一定的约束条件下，目标最优，损失最小。

此外，由于这个问题的特殊结构，还可以通过拉格朗日对偶性（Lagrange Duality）变换到对偶变量 (dual variable) 的优化问题，即通过求解与原问题等价的对偶问题（dual problem）得到原始问题的最优解，这就是线性可分条件下支持向量机的对偶算法，这样做的优点在于：一者对偶问题往往更容易求解；二者可以自然的引入核函数，进而推广到非线性分类问题。

那什么是拉格朗日对偶性呢？简单来讲，通过给每一个约束条件加上一个拉格朗日乘子（Lagrange multiplier），定义拉格朗日函数（通过拉格朗日函数将约束条件融合到目标函数里去，从而只用一个函数表达式便能清楚的表达出我们的问题）：
![](https://img-blog.csdnimg.cn/img_convert/f7a77562a2d65001b13b9893ce4821ea.png)
然后令![](https://img-blog.csdnimg.cn/img_convert/2e6ad2ab2bcf425e3825f95cbb677b0a.png)
容易验证，当某个约束条件不满足时，例如![](https://img-blog.csdn.net/20131107202615937)，那么显然有![](https://img-blog.csdn.net/20131107202642843)（只要令![](https://img-blog.csdn.net/20131107202702265)即可）。而当所有约束条件都满足时，则最优值为![](https://img-blog.csdn.net/20131111195433031)，亦即最初要最小化的量。

因此，在要求约束条件得到满足的情况下最小化![](https://img-blog.csdn.net/20131111195324546)，实际上等价于直接最小化（当然，这里也有约束条件，就是≥0,i=1,…,n）   ，因为如果约束条件没有得到满足，会等于无穷大，自然不会是我们所要求的最小值。

具体写出来，目标函数变成了：![](https://img-blog.csdnimg.cn/img_convert/b5028b57e2b99558109ef4b72e6243f8.png)

这里用![](https://img-blog.csdn.net/20131107202721703)表示这个问题的最优值，且和最初的问题是等价的。如果直接求解，那么一上来便得面对w和b两个参数，而![](https://img-blog.csdn.net/20131111195824031)又是不等式约束，这个求解过程不好做。不妨把最小和最大的位置交换一下，变成：![](https://img-blog.csdnimg.cn/img_convert/408957dc4c48bb283a80af781acce4f8.png)
交换以后的新问题是原始问题的对偶问题，这个新问题的最优值用 $d^*$ 来表示。而且有 $d^*$≤$p^*$，在满足某些条件的情况下，这两者相等，这个时候就可以通过求解对偶问题来间接地求解原始问题。

换言之，之所以从minmax的原始问题 $p^*$，转化为maxmin的对偶问题 $d^*$，一者因为 $d^*$ 是 $p^*$ 的近似解，二者，转化为对偶问题后，更容易求解。

下面可以先求 $L$ 对 $w$、$b$ 的极小，再求 $L$ 对 $\alpha$ 的极大。
#### 2.1.2、KKT条件
上文中提到 $d^*$ ≤ $p^*$ 在满足某些条件的情况下，两者等价”，这所谓的“满足某些条件”就是要满足KKT条件。
  
> 勘误：经大佬指出，这里的条件不应该是KKT条件，要让两者等价需满足strong duality （强对偶），而后有学者在强对偶下提出了KKT条件，且KKT条件的成立要
> 满足constraint qualifications，而constraint qualifications之一就是Slater条件。所谓Slater 条件，即指：凸优化问题，如果存在一个点x，使得所有等式约束> 都成立，并且所有不等式约束都严格成立（即取严格不等号，而非等号），则满足Slater 条件。对于此处，Slater 条件成立，所以d*≤p*可以取等号。

一般地，一个最优化数学模型能够表示成下列标准形式：![](https://img-blog.csdnimg.cn/20190127114042574.jpg)

其中, $f(x)$ 是需要最小化的函数，$h(x)$ 是等式约束，$g(x)$ 是不等式约束，$p$ 和 $q$ 分别为等式约束和不等式约束的数量。

同时，得明白以下两点：
+ 凸优化的概念： ![](https://img-blog.csdnimg.cn/img_convert/edb639ab7ad685a1ffe4afd247762327.png)为一凸集,![](https://img-blog.csdnimg.cn/img_convert/73106cdebc46629055781476a5342ee2.png)为一凸函数。凸优化就是要找出一点![](https://img-blog.csdnimg.cn/img_convert/ace0a3cf007573667f7d6bd1e1dfa4a0.gif)，使得每一![](https://img-blog.csdnimg.cn/img_convert/5329e127ec3fe0973c19d58a2ccab783.png)满足![](https://img-blog.csdnimg.cn/img_convert/f2c52c380cfdc9d57637baec7b582b57.gif)。
+ KKT条件的意义：它是一个非线性规划（Nonlinear Programming）问题能有最优化解法的必要和充分条件。

而KKT条件就是指上面最优化数学模型的标准形式中的最小点 $x*$ 必须满足下面的条件：![](https://img-blog.csdnimg.cn/20190127114104532.jpg)

经过论证，我们这里的问题是满足 KKT 条件的（首先已经满足Slater条件，再者 $f$ 和 $gi$ 也都是可微的，即 $L$ 对 $w$ 和 $b$ 都可导），因此现在我们便转化为求解第二个问题。

也就是说，原始问题通过满足KKT条件，已经转化成了对偶问题。而求解这个对偶学习问题，分为3个步骤：首先要让 $L(w，b，a)$ 关于 $w$ 和 $b$ 最小化，然后求对 $\alpha$ 的极大，最后利用SMO算法求解对偶问题中的拉格朗日乘子。

#### 2.1.3、对偶问题求解的3个步骤
（1）、首先固定 $\alpha$，要让 $L$ 关于 $w$ 和 $b$ 最小化，我们分别对 $w$，$b$ 求偏导数，即令 $∂L/∂w$ 和 $∂L/∂b$ 等于零:![](https://img-blog.csdn.net/20131107202220500)
将以上结果代入之前的 $L$:![](https://img-blog.csdnimg.cn/img_convert/f7a77562a2d65001b13b9893ce4821ea.png)

得到：![](https://img-blog.csdnimg.cn/img_convert/932c52a368509fecfab8dc4c0061db38.gif)

详细的推导过程：
![](https://img-blog.csdnimg.cn/img_convert/ce6d86e5c23fbe27648c920d673018a1.png)

最后，得到：
![](https://img-blog.csdnimg.cn/img_convert/932c52a368509fecfab8dc4c0061db38.gif)

从上面的最后一个式子，我们可以看出，此时的拉格朗日函数只包含了一个变量，那就是 $\alpha_i$（求出了便能求出 $w$，和 $b$，由此可见，上文第1.2节提出来的核心问题：分类函数 $f(x)=w^Tx+b$ 也就可以轻而易举的求出来了）

（2）、求对 $\alpha$ 的极大，即是关于对偶问题的最优化问题。经过上面第一个步骤的求 $w$ 和 $b$，得到的拉格朗日函数式子已经没有了变量 $w$，$b$，只有 $\alpha$。从上面的式子得到：
![](https://img-blog.csdnimg.cn/20190127114132410.jpg)

这样，求出了 $\alpha_i$，根据![](https://img-blog.csdnimg.cn/img_convert/3c7ff505da3ac403fef3ab8abea2c157.png)，即可求出 $w$，然后通过![](https://img-blog.csdnimg.cn/img_convert/37b7a6d986ea431612795495bc5b3eb5.png)，即可求出 $b$，最终得出分离超平面和分类决策函数。

（3）在求得 $L(w, b, a)$ 关于 $w$ 和 $b$ 最小化，以及对 $\alpha$ 的极大之后，最后一步则可以利用SMO算法求解对偶问题中的拉格朗日乘子 $\alpha$。
![](https://img-blog.csdnimg.cn/20190127114239385.jpg)

上述式子要解决的是在参数![](https://img-blog.csdnimg.cn/img_convert/7ffd65fae0490ba845b6c9f730f8d776.png)上求最大值 $W$ 的问题，至于 $x^{(i)}$ 和 $y^{(i)}$ 都是已知数。要了解这个SMO算法是如何推导的，请跳到下文第3.5节、SMO算法。

到目前为止，我们的 SVM 还比较弱，只能处理线性的情况，下面我们将引入核函数，进而推广到非线性分类问题。

#### 2.1.4、线性不可分的情况

对于一个数据点 $x$ 进行分类，实际上是通过把 $x$ 带入 $f(x)=w^Tx+b$ 到算出结果然后根据其正负号来进行类别划分的。而前面的推导中我们得到![](https://img-blog.csdn.net/20131111163543781) 
因此分类函数为：![](https://img-blog.csdnimg.cn/img_convert/d4418d12f4d8e5180212e55cde835bff.png)

这里的形式的有趣之处在于，对于新点 $x$ 的预测，只需要计算它与训练数据点的内积即可（![](https://img-blog.csdn.net/20131111163753093)表示向量内积），这一点至关重要，是之后使用 Kernel 进行非线性推广的基本前提。此外，所谓 Supporting Vector 也在这里显示出来——事实上，所有非Supporting Vector 所对应的系数 $\alpha$ 都是等于零的，因此对于新点的内积计算实际上只要针对少量的“支持向量”而不是所有的训练数据即可。

为什么非支持向量对应 $\alpha$ 的等于零呢？直观上来理解的话，就是这些“后方”的点——正如我们之前分析过的一样，对超平面是没有影响的，由于分类完全有超平面决定，所以这些无关的点并不会参与分类问题的计算，因而也就不会产生任何影响了。

回忆一下我们2.1.1节中通过 Lagrange multiplier得到的目标函数：![](https://img-blog.csdnimg.cn/img_convert/c0ae5fdc3189e4ef90926ce6aca55adc.png)

注意到如果 $x_i$ 是支持向量的话，上式中红颜色的部分是等于 0 的（因为支持向量的 functional margin 等于 1 ），而对于非支持向量来说，functional margin 会大于 1 ，因此红颜色部分是大于零的，而 $\alpha_i$ 又是非负的，为了满足最大化，$\alpha$ 必须等于 0 。这也就是这些非Supporting Vector 的点的局限性。 

至此，我们便得到了一个maximum margin hyper plane classifier，这就是所谓的支持向量机（Support Vector Machine）。当然，到目前为止，我们的 SVM 还比较弱，只能处理线性的情况，不过，在得到了对偶dual 形式之后，通过 Kernel 推广到非线性的情况就变成了一件非常容易的事情了(相信，你还记得本节开头所说的：“通过求解对偶问题得到最优解，这就是线性可分条件下支持向量机的对偶算法，这样做的优点在于：一者对偶问题往往更容易求解；二者可以自然的引入核函数，进而推广到非线性分类问题”)。

### 2.2、核函数Kernel
#### 2.2.1、特征空间的隐式映射：核函数

事实上，大部分时候数据并不是线性可分的，这个时候满足这样条件的超平面就根本不存在。在上文中，我们已经了解到了SVM处理线性可分的情况，那对于非线性的数据SVM咋处理呢？对于非线性的情况，SVM 的处理方法是选择一个核函数 $κ(⋅,⋅)$ ，通过将数据映射到高维空间，来解决在原始空间中线性不可分的问题。

具体来说，在线性不可分的情况下，支持向量机首先在低维空间中完成计算，然后通过核函数将输入空间映射到高维特征空间，最终在高维特征空间中构造出最优分离超平面，从而把平面上本身不好分的非线性数据分开。如图所示，一堆数据在二维空间无法划分，从而映射到三维空间里划分：

![](https://img-blog.csdn.net/20140830002108254)

而在我们遇到核函数之前，如果用原始的方法，那么在用线性学习器学习一个非线性关系，需要选择一个非线性特征集，并且将数据写成新的表达形式，这等价于应用一个固定的非线性映射，将数据映射到特征空间，在特征空间中使用线性学习器，因此，考虑的假设集是这种类型的函数：
![](https://img-blog.csdnimg.cn/20190127114318959.JPG)

这里 $ϕ：X->F$ 是从输入空间到某个特征空间的映射，这意味着建立非线性学习器分为两步：

+ 首先使用一个非线性映射将数据变换到一个特征空间F，
+ 然后在特征空间使用线性学习器分类。

而由于对偶形式就是线性学习器的一个重要性质，这意味着假设可以表达为训练点的线性组合，因此决策规则可以用测试点和训练点的内积来表示：
![](https://img-blog.csdnimg.cn/20190127114343476.JPG)

如果有一种方式可以在特征空间中直接计算内积 $〈φ(xi · φ(x)〉$，就像在原始输入点的函数中一样，就有可能将两个步骤融合到一起建立一个非线性的学习器，这样直接计算法的方法称为核函数方法：
核是一个函数K，对所有 $x，z\in X$，满足，这里 $φ$ 是从 $X$ 到内积特征空间F的映射。

#### 2.2.2、核函数：如何处理非线性数据
来看个核函数的例子。如下图所示的两类数据，分别分布为两个圆圈的形状，这样的数据本身就是线性不可分的，此时咱们该如何把这两类数据分开呢(下文将会有一个相应的三维空间图)？
![](https://img-blog.csdnimg.cn/2019012711441777.png)

事实上，上图所述的这个数据集，是用两个半径不同的圆圈加上了少量的噪音生成得到的，所以，一个理想的分界应该是一个“圆圈”而不是一条线（超平面）。如果用和来表示这个二维平面的两个坐标的话，我们知道一条二次曲线（圆圈是二次曲线的一种特殊情况）的方程可以写作这样的形式：
![](https://img-blog.csdn.net/20130820145508875)

注意上面的形式，如果我们构造另外一个五维的空间，其中五个坐标的值分别为 $Z_1=X_1$,$Z_2=X^2_1$ ,$Z_3=X_2$ ,$Z_4=X^2_2$ ，$Z_5=X_1X_2$，那么显然，上面的方程在新的坐标系下可以写作：![](https://img-blog.csdn.net/20130820145522437)

关于新的坐标 $Z$，这正是一个 hyper plane 的方程！也就是说，如果我们做一个映射![](https://img-blog.csdn.net/20180515235946910)，将 $x$ 按照上面的规则映射为 $Z$，那么在新的空间中原来的数据将变成线性可分的，从而使用之前我们推导的线性分类算法就可以进行处理了。这正是 Kernel 方法处理非线性问题的基本思想。

再进一步描述 Kernel 的细节之前，不妨再来看看上述例子在映射过后的直观形态。当然，你我可能无法把 5 维空间画出来，不过由于我这里生成数据的时候用了特殊的情形，所以这里的超平面实际的方程是这个样子的（圆心在 $X_2$ 轴上的一个正圆）：![](https://img-blog.csdnimg.cn/20210726210303260.png)

因此我只需要把它映射到 $Z_1=X_2^2$，$Z_2=X_2^2$，$Z_3=X_2$ 这样一个三维空间中即可，下图即是映射之后的结果，将坐标轴经过适当的旋转，就可以很明显地看出，数据是可以通过一个平面来分开的(pluskid：下面的gif 动画，先用 Matlab 画出一张张图片，再用 Imagemagick 拼贴成)：
![](https://img-blog.csdnimg.cn/img_convert/0e7fe11231baacb1a35c2a98756e042b.gif)

核函数相当于把原来的分类函数：![](https://img-blog.csdnimg.cn/img_convert/2d1e36719bcab86faf62309001a60e46.png)

映射成：![](https://img-blog.csdnimg.cn/img_convert/89c3a622d5a176eb4e44784be81f9e9d.png)

而其中的 $a$ 可以通过求解如下 dual 问题而得到的：
![](https://img-blog.csdnimg.cn/img_convert/8213e7ebe3fdee010446f16afb78cf67.png)


这样一来问题就解决了吗？似乎是的：拿到非线性数据，就找一个映射![](https://img-blog.csdnimg.cn/img_convert/c3d54d2e7635ff3c3259c1dbacbbb5ab.png)，然后一股脑把原来的数据映射到新空间中，再做线性 SVM 即可。不过事实上好像并没有这么简单。

细想一下，刚才的方法是不是有问题？

+ 在最初的例子里，我们对一个二维空间做映射，选择的新空间是原始空间的所有一阶和二阶的组合，得到了五个维度；
+ 如果原始空间是三维（一阶、二阶和三阶的组合），那么我们会得到：3(一次)+3(二次交叉)+3(平方)+3(立方)+1(x1*x2*x3)+2*3(交叉，一个一次一个二次，类似x1*x2^2) = 19维的新空间，这个数目是呈指数级爆炸性增长的，从而势必这给![](https://img-blog.csdnimg.cn/img_convert/c3d54d2e7635ff3c3259c1dbacbbb5ab.png)的计算带来非常大的困难，而且如果遇到无穷维的情况，就根本无从计算了。

这个时候，可能就需要 Kernel 出马了。

不妨还是从最开始的简单例子出发，设两个向量![](https://img-blog.csdnimg.cn/img_convert/43df5ce5fffcced9fc833e1c00c38962.png)和![](https://img-blog.csdnimg.cn/img_convert/7d312a6cf3e74f516dc044b1586cc5ed.png)，而![](https://img-blog.csdnimg.cn/img_convert/c3d54d2e7635ff3c3259c1dbacbbb5ab.png)即是到前面说的五维空间的映射，因此映射过后的内积为：![](https://img-blog.csdnimg.cn/img_convert/b4c4cb0e3cf574cda7b1ea45ff584d76.png)

（公式说明：上面的这两个推导过程中，所说的前面的五维空间的映射，这里说的前面便是文中2.2.1节的所述的映射方式，回顾下之前的映射规则，再看那第一个推导，其实就是计算x1，x2各自的内积，然后相乘相加即可，第二个推导则是直接平方，去掉括号，也很容易推出来）

另外，我们又注意到：
![](https://img-blog.csdnimg.cn/img_convert/c2c833330d286ebef989cb520db5d19e.png)

二者有很多相似的地方，实际上，我们只要把某几个维度线性缩放一下，然后再加上一个常数维度，具体来说，上面这个式子的计算结果实际上和映射
![](https://img-blog.csdnimg.cn/img_convert/eac5fd6b2207d7454586fa27ea9f4cbd.png)

之后的内积![](https://img-blog.csdnimg.cn/img_convert/8c22bfaf115c99024d6bc7f2a50f4047.png)的结果是相等的，那么区别在于什么地方呢？

+ 一个是映射到高维空间中，然后再根据内积的公式进行计算；
+ 而另一个则直接在原来的低维空间中进行计算，而不需要显式地写出映射后的结果。

（公式说明：上面之中，最后的两个式子，第一个算式，是带内积的完全平方式，可以拆开，然后，通过凑一个得到，第二个算式，也是根据第一个算式凑出来的）
回忆刚才提到的映射的维度爆炸，在前一种方法已经无法计算的情况下，后一种方法却依旧能从容处理，甚至是无穷维度的情况也没有问题。
我们把这里的计算两个向量在隐式映射过后的空间中的内积的函数叫做核函数 (Kernel Function) ，例如，在刚才的例子中，我们的核函数为：
![](https://img-blog.csdnimg.cn/img_convert/d16f103701156748b2392f55937f0088.png)

核函数能简化映射空间中的内积运算——刚好“碰巧”的是，在我们的 SVM 里需要计算的地方数据向量总是以内积的形式出现的。对比刚才我们上面写出来的式子，现在我们的分类函数为：![](https://img-blog.csdnimg.cn/img_convert/c8d75da618965746d2e78239f4f3195e.png)

其中 $a$ 由如下 dual 问题计算而得：
![](https://img-blog.csdnimg.cn/img_convert/64a73c4aa88ee9c95155aa7ef66ca45f.png)

这样一来计算的问题就算解决了，避开了直接在高维空间中进行计算，而结果却是等价的！当然，因为我们这里的例子非常简单，所以我可以手工构造出对应于![](https://img-blog.csdnimg.cn/img_convert/c3d54d2e7635ff3c3259c1dbacbbb5ab.png)的核函数出来，如果对于任意一个映射，想要构造出对应的核函数就很困难了。

#### 2.2.3、几个核函数

通常人们会从一些常用的核函数中选择（根据问题和数据的不同，选择不同的参数，实际上就是得到了不同的核函数），例如：

+ 多项式核，显然刚才我们举的例子是这里多项式核的一个特例（R = 1，d = 2）。虽然比较麻烦，而且没有必要，不过这个核所对应的映射实际上是可以写出来的，该空间的维度是![](https://img-blog.csdnimg.cn/img_convert/b2040ace35232e0d4a6b2d539b568ca4.png)，其中 m 是原始空间的维度。

+ 高斯核![](https://img-blog.csdnimg.cn/img_convert/ba1d941bb376f9b40c86e09f1d9fe490.png)，这个核就是最开始提到过的会将原始空间映射为无穷维空间的那个家伙。不过，如果![](https://img-blog.csdnimg.cn/img_convert/d1e1599c4aa2eee3f11c5f5592e84c5f.png)选得很大的话，高次特征上的权重实际上衰减得非常快，所以实际上（数值上近似一下）相当于一个低维的子空间；反过来，如果![](https://img-blog.csdnimg.cn/img_convert/d1e1599c4aa2eee3f11c5f5592e84c5f.png)选得很小，则可以将任意的数据映射为线性可分——当然，这并不一定是好事，因为随之而来的可能是非常严重的过拟合问题。不过，总的来说，通过调控参数![](https://img-blog.csdnimg.cn/img_convert/d1e1599c4aa2eee3f11c5f5592e84c5f.png)，高斯核实际上具有相当高的灵活性，也是使用最广泛的核函数之一。下图所示的例子便是把低维线性不可分的数据通过高斯核函数映射到了高维空间：
![](https://img-blog.csdn.net/20130919095640250)

+ 线性核![](https://img-blog.csdnimg.cn/img_convert/6072ddca8c296933c747b6a954b7de88.png)，这实际上就是原始空间中的内积。这个核存在的主要目的是使得“映射后空间中的问题”和“映射前空间中的问题”两者在形式上统一起来了(意思是说，咱们有的时候，写代码，或写公式的时候，只要写个模板或通用表达式，然后再代入不同的核，便可以了，于此，便在形式上统一了起来，不用再分别写一个线性的，和一个非线性的)。

#### 2.2.4、核函数的本质

上面说了这么一大堆，读者可能还是没明白核函数到底是个什么东西？我再简要概括下，即以下三点：

+ 实际中，我们会经常遇到线性不可分的样例，此时，我们的常用做法是把样例特征映射到高维空间中去(如上文2.2节最开始的那幅图所示，映射到高维空间后，相关特征便被分开了，也就达到了分类的目的)；
+ 但进一步，如果凡是遇到线性不可分的样例，一律映射到高维空间，那么这个维度大小是会高到可怕的(如上文中19维乃至无穷维的例子)。那咋办呢？
+ 此时，核函数就隆重登场了，核函数的价值在于它虽然也是将特征进行从低维到高维的转换，但核函数绝就绝在它事先在低维上进行计算，而将实质上的分类效果表现在了高维上，也就如上文所说的避免了直接在高维空间中的复杂计算。

最后引用这里的一个例子举例说明下核函数解决非线性问题的直观效果。

假设现在你是一个农场主，圈养了一批牛群，但为预防狼群袭击牛群，你需要搭建一个篱笆来把牛群围起来。但是篱笆应该建在哪里呢？你很可能需要依据牛群和狼群的位置建立一个“分类器”，比较下图这几种不同的分类器，我们可以看到SVM完成了一个很完美的解决方案。
![](https://img-blog.csdn.net/20131121105410546)

这个例子从侧面简单说明了SVM使用非线性分类器的优势，而逻辑模式以及决策树模式都是使用了直线方法。
OK，不再做过多介绍了，对核函数有进一步兴趣的，还可以看看[此文](https://www.cnblogs.com/vivounicorn/archive/2010/12/13/1904720.html)。

### 2.3、使用松弛变量处理 outliers 方法
在本文第一节最开始讨论支持向量机的时候，我们就假定，数据是线性可分的，亦即我们可以找到一个可行的超平面将数据完全分开。后来为了处理非线性数据，在上文2.2节使用 Kernel 方法对原来的线性 SVM 进行了推广，使得非线性的的情况也能处理。虽然通过映射将原始数据映射到高维空间之后，能够线性分隔的概率大大增加，但是对于某些情况还是很难处理。

例如可能并不是因为数据本身是非线性结构的，而只是因为数据有噪音。对于这种偏离正常位置很远的数据点，我们称之为 outlier ，在我们原来的 SVM 模型里，outlier 的存在有可能造成很大的影响，因为超平面本身就是只有少数几个 support vector 组成的，如果这些 support vector 里又存在 outlier 的话，其影响就很大了。例如下图：
![](https://img-blog.csdnimg.cn/img_convert/7df8cfee018256cea78d9a21c0ed8d56.png)

用黑圈圈起来的那个蓝点是一个 outlier ，它偏离了自己原本所应该在的那个半空间，如果直接忽略掉它的话，原来的分隔超平面还是挺好的，但是由于这个 outlier 的出现，导致分隔超平面不得不被挤歪了，变成途中黑色虚线所示（这只是一个示意图，并没有严格计算精确坐标），同时 margin 也相应变小了。当然，更严重的情况是，如果这个 outlier 再往右上移动一些距离的话，我们将无法构造出能将数据分开的超平面来。

为了处理这种情况，SVM 允许数据点在一定程度上偏离一下超平面。例如上图中，黑色实线所对应的距离，就是该 outlier 偏离的距离，如果把它移动回来，就刚好落在原来的 超平面 蓝色间隔边界上，而不会使得超平面发生变形了。
换言之，在有松弛的情况下outline点也属于支持向量SV，同时，对于不同的支持向量，拉格朗日参数的值也不同，如此篇论文《Large Scale Machine Learning》中的下图所示：
![](https://img-blog.csdn.net/20131126154347453)
对于远离分类平面的点值为0；对于边缘上的点值在[0, 1/L]之间，其中，L为训练数据集个数，即数据集大小；对于outline数据和内部的数据值为1/L。

OK，继续回到咱们的问题。我们，原来的约束条件为：![](https://img-blog.csdnimg.cn/img_convert/874b26968d0b4b018889552752669398.png)
现在考虑到outlier问题，约束条件变成了：![](https://img-blog.csdnimg.cn/img_convert/9423de8af6b44d8a1226bf2ca009b376.png)

其中![](https://img-blog.csdnimg.cn/img_convert/a6d5665d5fbef8c714758166de92c8c2.png)称为松弛变量 (slack variable) ，对应数据点 $x_i$ 允许偏离的 functional margin 的量。当然，如果我们![](https://img-blog.csdnimg.cn/img_convert/aee11ccda4c1a092b897c2e7059ab198.png)运行任意大的话，那任意的超平面都是符合条件的了。所以，我们在原来的目标函数后面加上一项，使得这些![](https://img-blog.csdnimg.cn/img_convert/aee11ccda4c1a092b897c2e7059ab198.png)的总和也要最小：![](https://img-blog.csdnimg.cn/img_convert/0e9d6ca22ccdfd05e1a4aa2d4c393c75.png)

其中 $C$ 是一个参数，用于控制目标函数中两项（“寻找 margin 最大的超平面”和“保证数据点偏差量最小”）之间的权重。注意，其中![](https://img-blog.csdnimg.cn/img_convert/86b70c777930fe3193aca65e03db1e07.png)是需要优化的变量（之一），而 $C$ 是一个事先确定好的常量。完整地写出来是这个样子：

![](https://img-blog.csdnimg.cn/img_convert/bfde084990627638d56a357b5094ae4c.png)

用之前的方法将限制或约束条件加入到目标函数中，得到新的拉格朗日函数，如下所示：
![](https://img-blog.csdnimg.cn/img_convert/b85e07b303f7af3ca116932299fcb685.png)
分析方法和前面一样，转换为另一个问题之后，我们先让![](https://img-blog.csdnimg.cn/img_convert/f9af92dd96444af8d414a0af9d6ed384.png)针对 $w$、$b$和![](https://img-blog.csdnimg.cn/img_convert/a516dbf14ac948568d9af19729d8679f.png)最小化:

![](https://img-blog.csdnimg.cn/img_convert/9ba0457f9f865295eee6b78b436b26e2.png)

将 $w$ 带回![](https://img-blog.csdnimg.cn/img_convert/0d419da2ab065a427f177dbb60559346.png)并化简，得到和原来一样的目标函数：![](https://img-blog.csdnimg.cn/img_convert/31af8d627e21aef55e2f5572982d08ca.png)

不过，由于我们得到![](https://img-blog.csdnimg.cn/img_convert/dbe0cb259d8a6af7c89c619009cf5f00.png)而又有 $r_i\geq0$（作为 Lagrange multiplier 的条件），因此有 $a_i\leq C$，所以整个 dual 问题现在写作：
![](https://img-blog.csdnimg.cn/img_convert/2ae4e8a310b75c0665347894ddedf64d.png)

把前后的结果对比一下（错误修正：图中的Dual formulation中的Minimize应为maxmize）：
![](https://img-blog.csdn.net/20130919094508562)

可以看到唯一的区别就是现在 dual variable $a$ 多了一个上限 $C$ 。而 Kernel 化的非线性形式也是一样的，只要把![](https://img-blog.csdnimg.cn/img_convert/6e97a43196dff359939585acaddfad4c.png)换成![](https://img-blog.csdnimg.cn/img_convert/e0878a10a2aa4a27853d9449e1563743.png)即可。这样一来，一个完整的，可以处理线性和非线性并能容忍噪音和 outliers 的支持向量机才终于介绍完毕了。

行文至此，可以做个小结，不准确的说，SVM它本质上即是一个分类方法，用 $w^T+b$ 定义分类函数，于是求w、b，为寻最大间隔，引出1/2||w||^2，继而引入拉格朗日因子，化为对拉格朗日乘子a的求解（求解过程中会涉及到一系列最优化或凸二次规划等问题），如此，求w.b与求a等价，而a的求解可以用一种快速学习算法SMO，至于核函数，是为处理非线性情况，若直接映射到高维计算恐维度爆炸，故在低维计算，等效高维表现。

### 3、证明SVM
### 3.1、线性学习器
#### 3.1.1、感知机算法

这个感知机算法是1956年提出的，年代久远，依然影响着当今，当然，可以肯定的是，此算法亦非最优，后续会有更详尽阐述。不过，有一点，你必须清楚，这个算法是为了干嘛的：不断的训练试错以期寻找一个合适的超平面(是的，就这么简单)。
![](https://img-blog.csdnimg.cn/20190127114747923.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3ZfSlVMWV92,size_16,color_FFFFFF,t_70)

下面，举个例子。如下图所示，凭我们的直觉可以看出，图中的红线是最优超平面，蓝线则是根据感知机算法在不断的训练中，最终，若蓝线能通过不断的训练移动到红线位置上，则代表训练成功。
![](https://img-blog.csdn.net/20131114203144875)

既然需要通过不断的训练以让蓝线最终成为最优分类超平面，那么，到底需要训练多少次呢？Novikoff定理告诉我们当间隔是正的时候感知机算法会在有限次数的迭代中收敛，也就是说Novikoff定理证明了感知机算法的收敛性，即能得到一个界，不至于无穷循环下去。
+ Novikoff定理：如果分类超平面存在, 仅需在序列S上迭代几次，在界为![](https://img-blog.csdn.net/20131114204028093)的错误次数下就可以找到分类超平面，算法停止。
这里![](https://img-blog.csdn.net/20131114204038984)，$\gamma$ 为扩充间隔。根据误分次数公式可知, 迭代次数与对应于扩充(包括偏置)权重的训练集的间隔有关。顺便再解释下这个所谓的扩充间隔 $\gamma$，$\gamma$ 即为样本到分类间隔的距离，即从 $\gamma$ 引出的最大分类间隔。OK，还记得上文第1.3节开头的内容么？如下：
![](https://img-blog.csdnimg.cn/img_convert/e1cc403aa9a3ced1d7b8f377c064a50f.png)

在给出几何间隔的定义之前，咱们首先来看下，如上图所示，对于一个点 $x$，令其垂直投影到超平面上的对应的为 $x_0$，由于w是垂直于超平面的一个向量，$\gamma$ 为样本 $x$ 到分类间隔的距离，我们有![](https://img-blog.csdn.net/20131107201720515)

然后后续怎么推导出最大分类间隔请回到本文第一、二部分。

同时有一点得注意：感知机算法虽然可以通过简单迭代对线性可分数据生成正确分类的超平面，但不是最优效果，那怎样才能得到最优效果呢，就是上文中第一部分所讲的寻找最大分类间隔超平面。此外，Novikoff定理的证明请见[这里](http://www.cs.columbia.edu/~mcollins/courses/6998-2012/notes/perc.converge.pdf)。

### 3.2、非线性学习器
#### 3.2.1、Mercer定理
Mercer定理 ：如果函数K是![](https://img-blog.csdn.net/20131114211056687)上的映射（也就是从两个n维向量映射到实数域）。那么如果K是一个有效核函数（也称为Mercer核函数），那么当且仅当对于训练样例![](https://img-blog.csdn.net/20131114211111875)，其相应的核函数矩阵是对称半正定的。 

### 3.3、损失函数
监督学习实际上就是一个经验风险或者结构风险函数的最优化问题。风险函数度量平均意义下模型预测的好坏，模型每一次预测的好坏用损失函数来度量。它从假设空间 $F$ 中选择模型 $f$ 作为决策函数，对于给定的输入 $X$，由 $f(X)$ 给出相应的输出 $Y$，这个输出的预测值 $f(X)$ 与真实值 $Y$ 可能一致也可能不一致，用一个损失函数来度量预测错误的程度。损失函数记为 $L(Y, f(X))$。
常用的损失函数有以下几种（基本引用自《统计学习方法》）：
![](https://img-blog.csdn.net/20131119171556593)
![](https://img-blog.csdn.net/20131119171613656)

如此，SVM有第二种理解，即最优化+损失最小。

OK，关于更多统计学习方法的问题，请参看[此文](https://blog.csdn.net/qll125596718/article/details/8351337)

### 3.4、最小二乘法
#### 3.4.1、什么是最小二乘法？

我们口头中经常说：一般来说，平均来说。如平均来说，不吸烟的健康优于吸烟者，之所以要加“平均”二字，是因为凡事皆有例外，总存在某个特别的人他吸烟但由于经常锻炼所以他的健康状况可能会优于他身边不吸烟的朋友。而最小二乘法的一个最简单的例子便是算术平均。

最小二乘法（又称最小平方法）是一种数学优化技术。它通过最小化误差的平方和寻找数据的最佳函数匹配。利用最小二乘法可以简便地求得未知的数据，并使得这些求得的数据与实际数据之间误差的平方和为最小。用函数表示为:![](https://img-blog.csdnimg.cn/img_convert/0d0470d79401a535c5c03b2214bdbbb3.png)

使误差「所谓误差，当然是观察值与实际真实值的差量」平方和达到最小以寻求估计值的方法，就叫做最小二乘法，用最小二乘法得到的估计，叫做最小二乘估计。当然，取平方和作为目标函数只是众多可取的方法之一。
最小二乘法的一般形式可表示为：![](https://img-blog.csdnimg.cn/img_convert/41235ddb8cc955e19404cde02bdc735f.png)

我们求解出导致累积误差最小的参数即可：![](https://img-blog.csdn.net/20131111200941109)

#### 3.4.2、最小二乘法的解法

什么是一元线性模型呢？ 请允许我[引用](https://blog.csdn.net/qll125596718/article/details/8248249)这里的内容，先来梳理下几个基本概念：

+ 监督学习中，如果预测的变量是离散的，我们称其为分类（如决策树，支持向量机等），如果预测的变量是连续的，我们称其为回归。
+ 回归分析中，如果只包括一个自变量和一个因变量，且二者的关系可用一条直线近似表示，这种回归分析称为一元线性回归分析。
+ 如果回归分析中包括两个或两个以上的自变量，且因变量和自变量之间是线性关系，则称为多元线性回归分析。
+ 对于二维空间线性是一条直线；对于三维空间线性是一个平面，对于多维空间线性是一个超平面...   

对于一元线性回归模型, 假设从总体中获取了n组观察值（X1，Y1），（X2，Y2）， …，（Xn，Yn）。对于平面中的这n个点，可以使用无数条曲线来拟合。要求样本回归函数尽可能好地拟合这组值。综合起来看，这条直线处于样本数据的中心位置最合理。 

选择最佳拟合曲线的标准可以确定为：使总的拟合误差（即总残差）达到最小。有以下三个标准可以选择：        

+ 用“残差和最小”确定直线位置是一个途径。但很快发现计算“残差和”存在相互抵消的问题。
+ 用“残差绝对值和最小”确定直线位置也是一个途径。但绝对值的计算比较麻烦。
+ 最小二乘法的原则是以“残差平方和最小”确定直线位置。用最小二乘法除了计算比较方便外，得到的估计量还具有优良特性。这种方法对异常值非常敏感。　 
最常用的是普通最小二乘法（ Ordinary  Least Square，OLS）：所选择的回归模型应该使所有观察值的残差平方和达到最小，即采用平方损失函数。 　
我们定义样本回归模型为：![](https://img-blog.csdnimg.cn/img_convert/08a8a8668605a79516a0f4b228fab98b.gif)

其中ei为样本（Xi, Yi）的误差。
接着，定义平方损失函数Q：![](https://img-blog.csdnimg.cn/img_convert/a34e447fd778ef1bf788839eb894a969.gif)

则通过Q最小确定这条直线，即确定，为变量，把它们看作是Q的函数，就变成了一个求极值的问题，可以通过求导数得到。
求Q对两个待估参数的偏导数：
![](https://img-blog.csdnimg.cn/img_convert/90bfb8e286c392066790c76ecb364e11.gif)

根据数学知识我们知道，函数的极值点为偏导为0的点。解得：
![](https://img-blog.csdnimg.cn/img_convert/c23c60b12d55c4c6bad5b7e09140a5d0.gif)

这就是最小二乘法的解法，就是求得平方损失函数的极值点。自此，你看到求解最小二乘法与求解SVM问题何等相似，尤其是定义损失函数，而后通过偏导求得极值。
```c++
#include<iostream>
#include<fstream>
#include<vector>
using namespace std;
 
class LeastSquare{
	double a, b;
public:
	LeastSquare(const vector<double>& x, const vector<double>& y)
	{
		double t1=0, t2=0, t3=0, t4=0;
		for(int i=0; i<x.size(); ++i)
		{
			t1 += x[i]*x[i];
			t2 += x[i];
			t3 += x[i]*y[i];
			t4 += y[i];
		}
		a = (t3*x.size() - t2*t4) / (t1*x.size() - t2*t2);
		//b = (t4 - a*t2) / x.size();
		b = (t1*t4 - t2*t3) / (t1*x.size() - t2*t2);
	}
 
	double getY(const double x) const
	{
		return a*x + b;
	}
 
	void print() const
	{
		cout<<"y = "<<a<<"x + "<<b<<"\n";
	}
 
};
 
int main(int argc, char *argv[])
{
	if(argc != 2)
	{
		cout<<"Usage: DataFile.txt"<<endl;
		return -1;
	}
	else
	{
		vector<double> x;
		ifstream in(argv[1]);
		for(double d; in>>d; )
			x.push_back(d);
		int sz = x.size();
		vector<double> y(x.begin()+sz/2, x.end());
		x.resize(sz/2);
		LeastSquare ls(x, y);
		ls.print();
		
		cout<<"Input x:\n";
		double x0;
		while(cin>>x0)
		{
			cout<<"y = "<<ls.getY(x0)<<endl;
			cout<<"Input x:\n";
		}
	}
}
```
#### 3.5.1、SMO算法的推导
咱们首先来定义特征到结果的输出函数：
![](https://img-blog.csdn.net/20131120103601656)

注：这个u与我们之前定义的实质是一样的。
接着，重新定义下咱们原始的优化问题，权当重新回顾，如下：
![](https://img-blog.csdn.net/20131120103959046)
求导得到：![](https://img-blog.csdn.net/20131120104205453)
代入![](https://img-blog.csdn.net/20131120103601656)中，可得![](https://img-blog.csdn.net/20131120105420562)

通过引入拉格朗日乘子转换为对偶问题后，得：![](https://img-blog.csdn.net/20131120104258437)
![](https://img-blog.csdn.net/20131120104335796)
且![](https://img-blog.csdn.net/20131120104344000)

注：这里得到的min函数与我们之前的max函数实质也是一样，因为把符号变下，即由min转化为max的问题，且 $y_i$ 也与之前 $y^{(i)}$ 的等价，$y_j$亦如此。

经过加入松弛变量后，模型修改为：

![](https://img-blog.csdn.net/20131120105112312)
![](https://img-blog.csdn.net/20131120105326156)

从而最终我们的问题变为：
![](https://img-blog.csdn.net/20131120105753218)

下面要解决的问题是：在![](https://img-blog.csdn.net/20140917223126859)上求上述目标函数的最小值。为了求解这些乘子，每次从中任意抽取两个乘子$a_1$和$a_2$，然后固定和以外的其它乘子![](https://img-blog.csdn.net/20140917223413501)，使得目标函数只是关于$a_1$和$a_2$的函数。这样，不断的从一堆乘子中任意抽取两个求解，不断的迭代求解子问题，最终达到求解原问题的目的。

而原对偶问题的子问题的目标函数可以表达为：![](https://img-blog.csdn.net/20140909162540829)
其中![](https://img-blog.csdn.net/20140909162911265)

为了解决这个子问题，首要问题便是每次如何选取$a_1$和$a_2$。实际上，其中一个乘子是违法KKT条件最严重的，另外一个乘子则由另一个约束条件选取
根据KKT条件可以得出目标函数中$a_i$取值的意义：
![](https://img-blog.csdn.net/20131120105843187)
这里的$a_i$还是拉格朗日乘子：

+ 对于第1种情况，表明$a_i$是正常分类，在间隔边界内部（我们知道正确分类的点![](https://img-blog.csdn.net/20140917223345468)）；
+ 对于第2种情况，表明$a_i$是支持向量，在间隔边界上；
+ 对于第3种情况，表明$a_i$是在两条间隔边界之间；

而最优解需要满足KKT条件，即上述3个条件都得满足，以下几种情况出现将会出现不满足：
+ $y_iu_i<=1$ 但是 $a_i<C$  则是不满足的，而原本 $a_i=C$
+ $y_iu_i>=1$ 但是 $a_i>0$ 则是不满足的，而原本 $a_i$ =0
+ $y_iu_i=1$ 但是 $a_i=0$ 或者 $a_i=C$ 则表明不满足的，而原本应该是 $0<a_i<C$
也就是说，如果存在不满足KKT条件的 $a_i$，那么需要更新这些 $a_i$，这是第一个约束条件。此外，更新的同时还要受到第二个约束条件的限制，即![](https://img-blog.csdn.net/20131120104344000)

因此，如果假设选择的两个乘子$a_1$和$a_2$，它们在更新之前分别是 $a_1^{old}$、$a_2^{old}$，更新之后分别是 $a_1^{new}$、$a_2^{new}$，那么更新前后的值需要满足以下等式才能保证和为0的约束：
![](https://img-blog.csdn.net/20140917211428859)其中，![](https://img-blog.csdn.net/20140917211751882)是常数。

两个因子不好同时求解，所以可先求第二个乘子 $a_2$ 的解（$a_2^{new}$），得到 $a_2$ 的解（$a_2^{new}$）之后，再用 $a_2$ 的解（$a_2^{new}$）表示 $a_1$ 的解（$a_1^{new}$）。

为了求解 $a_2^{new}$，得先确定 $a_2^{new}$ 的取值范围。假设它的上下边界分别为 $H$ 和 $L$，那么有：![](https://img-blog.csdn.net/20140917212018772)

接下来，综合![](https://img-blog.csdn.net/20140917212046369)和![](https://img-blog.csdn.net/20140917212056119)这两个约束条件，求取的 $a_2^{new}$ 取值范围。
当 $y1!=y2$ 时，根据![](https://img-blog.csdn.net/20140917212056119)可得![](https://img-blog.csdn.net/20140917223713557)，所以有![](https://img-blog.csdn.net/20141210233122343)，![](https://img-blog.csdn.net/20141210233135376)，如下图所示：
![](https://img-blog.csdn.net/20140917212349092)
当 $y1=y2$ 时，同样根据![](https://img-blog.csdn.net/20140917212056119)可得：![](https://img-blog.csdn.net/20140917223614125)，所以有![](https://img-blog.csdn.net/20141210233152921)，![](https://img-blog.csdn.net/20141210233202208)，如下图所示：

![](https://img-blog.csdn.net/20140917212546875)

如此，根据 $y_1$ 和 $y_2$ 异号或同号，可得出 $a_2^{new}$ 的上下界分别为：
![](https://img-blog.csdn.net/20140917212422859)
回顾下第二个约束条件![](https://img-blog.csdn.net/20140917212056119)，令上式两边乘以 $y_1$，可得![](https://img-blog.csdn.net/20140909164502391)
其中，![](https://img-blog.csdn.net/20140909164354343)。

因此 $a_1$ 可以用 $a_2$ 表示，$a_1=w-s*a_2$，从而把子问题的目标函数转换为只含 $a_2$ 的问题：
![](https://img-blog.csdn.net/20140909170457774)
对求 $a_2$ 导，可得![](https://img-blog.csdn.net/20140909170904988)
化简下：
![](https://img-blog.csdn.net/20140909170907640)

然后将 $s=y_1*y_2$、![](https://img-blog.csdn.net/20140909164502391)、![](https://img-blog.csdn.net/20140909162911265)和代入上式可得：![](https://img-blog.csdn.net/20140921114838239)

令![](https://img-blog.csdn.net/20131120102813671)（表示预测值与真实值之差），![](https://img-blog.csdn.net/20140917224103392)，然后上式两边同时除以 $\eta$，得到一个关于单变量的解 $a_2$：![](https://img-blog.csdn.net/20140917213317884)

这个解没有考虑其约束条件 ![](https://img-blog.csdn.net/20140917213244500)，即是未经剪辑时的解。
然后考虑约束![](https://img-blog.csdn.net/20140917213244500)可得到经过剪辑后的 $a_2^{new}$ 的解析解为：
![](https://img-blog.csdn.net/20140917213408953)
求出了后 $a_2^{new}$，便可以求出 $a_1^{new}$，得![](https://img-blog.csdn.net/20140917213703086)。
那么如何选择乘子 $a_1$ 和 $a_2$ 呢？
+ 对于 $a_1$，即第一个乘子，可以通过刚刚说的那3种不满足KKT的条件来找；
+ 而对于第二个乘子 $a_2$ 可以寻找满足条件：![](https://img-blog.csdn.net/20140917223934406)的乘子。
而 $b$ 在满足下述条件：
![](https://img-blog.csdn.net/20140917213613796)
下更新 $b$：

![](https://img-blog.csdn.net/20140917213709734)

![](https://img-blog.csdn.net/20140917213912504)

且每次更新完两个乘子的优化后，都需要再重新计算 $b$，及对应的 $Ei$ 值。
最后更新所有 $a_i$，$y$ 和 $b$，这样模型就出来了，从而即可求出咱们开头提出的分类函数：
![](https://img-blog.csdnimg.cn/img_convert/2d1e36719bcab86faf62309001a60e46.png)
此外，[这里](https://www.cnblogs.com/jerrylead/archive/2011/03/18/1988419.html)也有一篇类似的文章，大家可以参考下。

#### 3.5.2、SMO算法的步骤
综上，总结下SMO的主要步骤，如下：
![](https://img-blog.csdnimg.cn/img_convert/3ab64c7d1be717b35d6849af29b348db.png)

意思是，

+ 第一步选取一对 $a_i$ 和 $a_j$，选取方法使用启发式方法；
+ 第二步，固定除 $a_i$ 和 $a_j$ 之外的其他参数，确定 $W$ 极值条件下的 $a_i$，$a_j$ 由 $a_i$ 表示。
假定在某一次迭代中，需要更新 $X_1$，$X_2$ 对应的拉格朗日乘子 $a_1$，$a_2$，那么这个小规模的二次规划问题写为：
![](https://img-blog.csdn.net/20131106172007546)
那么在每次迭代中，如何更新乘子呢？引用这里的两张PPT说明下：
![](https://img-blog.csdn.net/20131106171714031)
![](https://img-blog.csdn.net/20131106171728703)

知道了如何更新乘子，那么选取哪些乘子进行更新呢？具体选择方法有以下两个步骤：

+ 步骤1：先“扫描”所有乘子，把第一个违反KKT条件的作为更新对象，令为 $a_1$；
+ 步骤2：在所有不违反KKT条件的乘子中，选择使 $|E1 −E2|$ 最大的 $a_2$ 进行更新，使得能最大限度增大目标函数的值（类似于梯度下降. 此外 $E_i=u_i-y_i$，而![](https://img-blog.csdn.net/20131120103601656)，求出来的 $E$ 代表函数 $u_i$ 对输入 $x_i$ 的预测值与真实输出类标记 $y_i$ 之差）。
最后，每次更新完两个乘子的优化后，都需要再重新计算 $b$，及对应的 $E_i$ 值。

综上，SMO算法的基本思想是将Vapnik在1982年提出的Chunking方法推到极致，SMO算法每次迭代只选出两个分量 $a_i$ 和 $a_j$ 进行调整，其它分量则保持固定不变，在得到解 $a_i$ 和 $a_j$ 之后，再用 $a_i$ 和 $a_j$改进其它分量。与通常的分解算法比较，尽管它可能需要更多的迭代次数，但每次迭代的计算量比较小，所以该算法表现出较好的快速收敛性，且不需要存储核矩阵，也没有矩阵运算。

